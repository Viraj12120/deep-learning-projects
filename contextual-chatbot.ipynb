{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e107d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334e2707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.53.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (58.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.22.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.22.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.0.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eafdaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tflearn in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tflearn) (1.22.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tflearn) (9.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tflearn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tflearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e6fa160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer=LancasterStemmer()\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95842931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing intents....\n"
     ]
    }
   ],
   "source": [
    "with open(\"intents.json\") as js_data:\n",
    "    intents=json.load(js_data)\n",
    "print(\"processing intents....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91677f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looping through intent to convert them to words,classes,documents,ignorewords....\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "documents=[]\n",
    "classes=[]\n",
    "ignore_words=[\"!\",\"?\"]\n",
    "print(\"looping through intent to convert them to words,classes,documents,ignorewords....\")\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize every word in the sentence\n",
    "        wi=nltk.word_tokenize(pattern)\n",
    "        #add to our word list\n",
    "        words.extend(wi)\n",
    "        #add to documents in the corpus\n",
    "        documents.append((wi,intent['tag']))\n",
    "        # add to class list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9fff4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowering stemming and removing duplicates..\n",
      "44 unique stemmed words [\"'s\", 'and', 'anyon', 'appoin', 'ar', 'assist', 'book', 'bye', 'can', 'cough', 'dat', 'day', 'fev', 'good', 'goodby', 'hav', 'hello', 'help', 'hi', 'hol', 'hour', 'how', 'i', 'is', 'lat', 'me', 'musc', 'my', 'nee', 'of', 'pain', 'pleas', 'see', 'stomach', 'thank', 'that', 'ther', 'tim', 'today', 'u', 'what', 'with', 'yo', 'you']\n",
      "23 documents\n",
      "7 classes ['book', 'goodbye', 'greeting', 'help', 'symptoms', 'thanks', 'today']\n"
     ]
    }
   ],
   "source": [
    "print('lowering stemming and removing duplicates..')\n",
    "words=[stemmer.stem(wi.lower())for wi in words if wi not in ignore_words]\n",
    "words=sorted(list(set(words)))\n",
    "\n",
    "#remove duplicates\n",
    "classes=sorted(list(set(classes)))\n",
    "print(len(words),'unique stemmed words',words)\n",
    "print(len(documents),\"documents\")\n",
    "print(len(classes),\"classes\",classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d34336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create data for the model...\n",
      "create empty list for output...\n",
      "create training set,bag of words for model\n"
     ]
    }
   ],
   "source": [
    "print(\"create data for the model...\")\n",
    "training=[]\n",
    "output=[]\n",
    "print(\"create empty list for output...\")\n",
    "output_empty=[0] * len(classes)\n",
    "print(\"create training set,bag of words for model\")\n",
    "for doc in documents:\n",
    "    #initialize bag of words\n",
    "    bag=[]\n",
    "    #list of tokenize words for pattern\n",
    "    pattern_words=doc[0]\n",
    "    #stem each word\n",
    "    pattern_words=[stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    for wi in words:\n",
    "        bag.append(1) if wi in pattern_words else bag.append(0)\n",
    "    # output 0 for each tag and 1 for current tag \n",
    "    output_row=list(output_empty)\n",
    "    output_row[classes.index(doc[1])]=1\n",
    "    \n",
    "    training.append([bag,output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ea712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling randomly and converting to numpy array for faster processing..\n",
      "building neural network for chatbot to contextual... \n",
      "resetting graph data \n"
     ]
    }
   ],
   "source": [
    "print(\"shuffling randomly and converting to numpy array for faster processing..\")\n",
    "random.shuffle(training)\n",
    "training=np.array(training)\n",
    "train_x=list(training[:,0])\n",
    "train_y=list(training[:,1])\n",
    "print(\"building neural network for chatbot to contextual... \")\n",
    "print(\"resetting graph data \")\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb928556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tflearn\\initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "training..\n"
     ]
    }
   ],
   "source": [
    "net=tflearn.input_data(shape=[None,len(train_x[0])])\n",
    "net=tflearn.fully_connected(net,8)\n",
    "net=tflearn.fully_connected(net,8)\n",
    "net=tflearn.fully_connected(net,len(train_y[0]), activation='softmax')\n",
    "net=tflearn.regression(net)\n",
    "print('training..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fed516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tflearn.DNN(net,tensorboard_dir=\"tflearn_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e39604fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.49723\u001b[0m\u001b[0m | time: 0.023s\n",
      "| Adam | epoch: 1000 | loss: 0.49723 - acc: 0.9449 -- iter: 16/23\n",
      "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.45614\u001b[0m\u001b[0m | time: 0.036s\n",
      "| Adam | epoch: 1000 | loss: 0.45614 - acc: 0.9504 -- iter: 23/23\n",
      "--\n",
      "saving the model...\n",
      "INFO:tensorflow:C:\\Users\\HP\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "print(\"training the model.....\")\n",
    "model.fit(train_x,train_y,n_epoch=1000,batch_size=8,show_metric=True)\n",
    "print(\"saving the model...\")\n",
    "model.save(\"model.tflearn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78913fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickel also saved.......\n"
     ]
    }
   ],
   "source": [
    "print(\"pickel also saved.......\")\n",
    "pickle.dump({\"classes\":classes,\"train_x\":train_x,\"train_y\":train_y,\"words\":words},open(\"training_data\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec091ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle is loaded..\n",
      "loading the model\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\HP\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "print(\"pickle is loaded..\")\n",
    "data=pickle.load(open('training_data','rb'))\n",
    "words=data['words']\n",
    "classes=data['classes']\n",
    "train_x=data['train_x']\n",
    "train_y=data['train_y']\n",
    "\n",
    "with open('intents.json') as js_data:\n",
    "    intents=json.load(js_data)\n",
    "    \n",
    "print('loading the model')   \n",
    "#load saved model\n",
    "model.load('./model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5ab94d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_Thresold=0.25\n"
     ]
    }
   ],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    #it tokenize or break constituents of sentence\n",
    "    sentence_words=nltk.word_tokenize(sentence)\n",
    "    #stemming=finding root of given word For eg: giving:stem-word=giv likewise\n",
    "    sentence_words=[stemmer.stem(word.lower())for word in sentence_words]\n",
    "    \n",
    "    #Return  the array of bag of words:True and False or 0 and 1 for each word present in the sentence:\n",
    "    def bow(sentence,words,show_details=False):\n",
    "        sentence_words=clean_up_sentence(sentence)\n",
    "        bag=[0]*len(words)\n",
    "        for s in sentence_words:\n",
    "            for i,w in enumerate(words):\n",
    "                if w==s:\n",
    "                    bag[i]=1\n",
    "                if show_details:\n",
    "                    print(\" found in bag: %s\"%w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "Error_Threshold=0.25\n",
    "print(\"Error_Thresold=0.25\")\n",
    "\n",
    "def classify(sentence):\n",
    "    #prediction of to get possibility or probability from the model:\n",
    "    results=model.predict([bow(sentence,words)])[0]\n",
    "    #exclude results which are below threshhold\n",
    "    results=[[i,r] for i,r in enumerate(results)if r>Error_Threshold]\n",
    "    #Sorting is done as higher confidence answer comes first\n",
    "    results.sort(key=lambda x:x[1],reverse=True)\n",
    "    return_list=[]\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]],r[1]))\n",
    "    return return_list\n",
    "\n",
    "def response(sentence,user_ID='123',show_details=False):\n",
    "    results=classify(sentence)\n",
    "    #That means if classification is done then find matching tag\n",
    "    if results:\n",
    "    #long loop\n",
    "       while results:\n",
    "            for i in intents['intents']:\n",
    "            #tag matching\n",
    "              if i['tag']==results[0][0]:\n",
    "            #random response from high probability order\n",
    "                 return  print(random.choice(i['responses']))\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e87ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    input_data=input(\"you-\")\n",
    "    answer=response(input_data)\n",
    "    answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e34e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
